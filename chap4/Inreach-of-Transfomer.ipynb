{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Pytorch中使用Transformer变换单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化一个Transformer编码器，每个输入向量的维度为4，头数为（指多头自注意力模型的头数）2\n",
    "encoder = nn.TransformerEncoderLayer(d_model=4,nhead=2,device=device)\n",
    "encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过观察结果可知Transformer结构为：  \n",
    "- 1个多头自注意力层\n",
    "- 两个隐含层执行两次隐式线性变换\n",
    "- 两层归一化层用于将隐含层中的4维向量转换为一维向量\n",
    "- 三层Droputput层用弃用一部分的神经元以防止过拟合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`nn.TransfomerEncoder`为`nn.TransformerEncoderLayer`进行批量编码器的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders = nn.TransformerEncoder(encoder_layer=encoder,num_layers=6)\n",
    "encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到`enocders`变量的类型为：`ModuleList`，即一个包含了多层的编码器的模型列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4290, 0.6809, 0.2100, 0.5406],\n",
       "         [0.3785, 0.8010, 0.8515, 0.6673],\n",
       "         [0.4872, 0.2514, 0.2640, 0.5209]],\n",
       "\n",
       "        [[0.3463, 0.2781, 0.8126, 0.1205],\n",
       "         [0.1959, 0.1769, 0.4507, 0.8872],\n",
       "         [0.2026, 0.6373, 0.6387, 0.8949]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化一个2*3*4的张量\n",
    "src = torch.rand(2,3,4,device=device)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7686,  1.3061, -1.1516,  0.6141],\n",
       "         [-1.4517,  0.1809, -0.0900,  1.3608],\n",
       "         [-1.1507, -0.2531, -0.2000,  1.6037]],\n",
       "\n",
       "        [[ 1.0709,  0.3352,  0.2330, -1.6391],\n",
       "         [-0.8468, -1.0739,  0.5829,  1.3378],\n",
       "         [-1.5669, -0.1624,  0.7569,  0.9724]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = encoders(src)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，我们可以使用`nn.TransformerDecoderLayer`和`nn.TransformerDecoder`来生成一个解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = torch.nn.TransformerDecoderLayer(d_model=4,nhead=2,device=device) # 解码器的参数规格应与编码器保持一致\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
       "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoders = torch.nn.TransformerDecoder(decoder_layer=decoder,num_layers=6)\n",
    "decoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意：在执行解码操作时，仍需准备一个输入张量的副本（输出张量）用于存储解码器的输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7686,  1.3061, -1.1516,  0.6141],\n",
       "         [-1.4517,  0.1809, -0.0900,  1.3608],\n",
       "         [-1.1507, -0.2531, -0.2000,  1.6037]],\n",
       "\n",
       "        [[ 1.0709,  0.3352,  0.2330, -1.6391],\n",
       "         [-0.8468, -1.0739,  0.5829,  1.3378],\n",
       "         [-1.5669, -0.1624,  0.7569,  0.9724]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_decoder = torch.rand(2,3,4,device=device)\n",
    "memory = output\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9508,  1.5854, -0.7538,  0.1191],\n",
       "         [ 0.7385, -0.8961,  1.2276, -1.0699],\n",
       "         [ 0.7304, -1.1609,  1.2218, -0.7913]],\n",
       "\n",
       "        [[-1.2124,  1.4994, -0.4905,  0.2035],\n",
       "         [ 0.7933, -1.3936,  1.0902, -0.4899],\n",
       "         [ 0.9058, -1.1722,  1.0739, -0.8075]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = decoders(outputs_decoder,memory)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
