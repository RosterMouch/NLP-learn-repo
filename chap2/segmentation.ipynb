{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lexicon.txt\", \"w\") as f:\n",
    "    words = f\"\"\"我\\n喜欢\\n读书\\n研究\\n研究生\\n生命\\n的\\n起源\"\"\"\n",
    "    f.write(words)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义正向最大匹配算法FMM\n",
    "def fmm_word_seg(sentence,lexicon,max_len):\n",
    "    \"\"\"\n",
    "    sentence: 待分词的句子\n",
    "    lexicon: 词典\n",
    "    max_len: 词典中最长词的长度\n",
    "    \"\"\"\n",
    "    begin = 0 # 设置句子的起始位置\n",
    "    end = min(begin+max_len,len(sentence)) # 设置句子的结束位置\n",
    "    word_list = [] # 用于存储分词结果\n",
    "    while begin < end:\n",
    "        word = sentence[begin:end] # 对句子进行截取\n",
    "        if word in lexicon or end - begin == 1: # 如果截取的词在词典中或者截取的词只有一个字\n",
    "            word_list.append(word) # 将该词添加到分词结果中\n",
    "            begin = end # 将句子的起始位置设置为结束位置\n",
    "            end = min(begin+max_len,len(sentence)) # 将句子的结束位置设置为最大词长和句子长度的最小值\n",
    "        else:\n",
    "            end -= 1 # 如果截取的词不在词典中，则将结束位置向前移动一个位置\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义词典加载函数\n",
    "def load_dict(fileNameString):\n",
    "    f = open(fileNameString,encoding=\"utf-8\")\n",
    "    lexicon = set() # 用于存储词典中的词的集合\n",
    "    max_len = 0 # 用于存储词典中最长词的长度\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        lexicon.add(word)\n",
    "        if len(word) > max_len: # 如果当前词的长度大于max_len，则更新max_len\n",
    "            max_len = len(word)\n",
    "    f.close()\n",
    "    return lexicon,max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '喜欢', '读书']\n"
     ]
    }
   ],
   "source": [
    "lexicon,max_len = load_dict(\"lexicon.txt\")\n",
    "sentence = \"我喜欢读书\"\n",
    "word_list = fmm_word_seg(sentence,lexicon,max_len)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究生', '命', '的', '起源']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"研究生命的起源\"\n",
    "word_list = fmm_word_seg(sentence,lexicon,max_len)\n",
    "print(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "000正向最大匹配分词算法存在的明显缺点是清扬御切分出较长的词，这种特性非常致命。  \n",
    "例如上例中的“研究生命的起源”被分割成了['研究生', '命', '的', '起源']，这显然是不符合预期的结果。  \n",
    "而这种问题被统称为`切分歧义`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
